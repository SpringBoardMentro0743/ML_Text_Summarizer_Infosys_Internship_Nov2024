{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qrbdoG0kS35",
        "outputId": "b1af633b-ae19-4916-eb8e-2421c5eb51f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initial zip file loading"
      ],
      "metadata": {
        "id": "5obgh3GpaXYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to ZIP file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/cnn_dailymail_dataset.zip'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "# Check if files are extracted correctly\n",
        "os.listdir('/content/data')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrdtPNOHkoHK",
        "outputId": "f2023a48-5a31-4091-dba4-1e3065e662fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__MACOSX', 'cnn_dailymail_dataset']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quality data segregation"
      ],
      "metadata": {
        "id": "k8LTpES1sGrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "X_Antm9PsVPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to articles and summaries\n",
        "articles_path = '/content/data/cnn_dailymail_dataset/Articles'\n",
        "summaries_path = '/content/data/cnn_dailymail_dataset/Summaries'\n",
        "\n",
        "# Lists to hold articles and summaries\n",
        "articles = []\n",
        "summaries = []\n",
        "\n",
        "# Read each file and store content\n",
        "for filename in tqdm(os.listdir(articles_path)):\n",
        "    with open(os.path.join(articles_path, filename), 'r', encoding='utf-8') as file:\n",
        "        articles.append(file.read())\n",
        "\n",
        "for filename in tqdm(os.listdir(summaries_path)):\n",
        "    with open(os.path.join(summaries_path, filename), 'r', encoding='utf-8') as file:\n",
        "        summaries.append(file.read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orW5H99klimZ",
        "outputId": "be1981e0-2684-444e-ce89-2f4f8acd4e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 287113/287113 [01:45<00:00, 2725.29it/s]\n",
            "100%|██████████| 287113/287113 [00:13<00:00, 21815.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'article': articles, 'summary': summaries})\n",
        "\n",
        "# Display the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IYyac0ZKlpil",
        "outputId": "c8d3aea6-22b8-4467-8100-3fb30bc6e1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             article  \\\n",
              "0  'Liberation didn't happen': Feminist academic ...   \n",
              "1  By . Peter Campbell and Tamara Cohen . PUBLISH...   \n",
              "2  Brazil's sports tribunal says Chile playmaker ...   \n",
              "3  By . Anna Hodgekiss . A woman given a vagina g...   \n",
              "4  By . Olivia Williams . PUBLISHED: . 15:17 EST,...   \n",
              "\n",
              "                                             summary  \n",
              "0  Carer Janet Maddocks, 57,  was caught red-hand...  \n",
              "1  Kate, 40, reveals her beauty tips and how to l...  \n",
              "2  CNN's Kara Devlin was diagnosed with hyperemes...  \n",
              "3  More seniors are making music in their golden ...  \n",
              "4  Jimmy Greaves did not play in the 1966 World C...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c86cb894-8d6f-4db3-9d14-ec13c159dc2b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'Liberation didn't happen': Feminist academic ...</td>\n",
              "      <td>Carer Janet Maddocks, 57,  was caught red-hand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>By . Peter Campbell and Tamara Cohen . PUBLISH...</td>\n",
              "      <td>Kate, 40, reveals her beauty tips and how to l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Brazil's sports tribunal says Chile playmaker ...</td>\n",
              "      <td>CNN's Kara Devlin was diagnosed with hyperemes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>By . Anna Hodgekiss . A woman given a vagina g...</td>\n",
              "      <td>More seniors are making music in their golden ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>By . Olivia Williams . PUBLISHED: . 15:17 EST,...</td>\n",
              "      <td>Jimmy Greaves did not play in the 1966 World C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c86cb894-8d6f-4db3-9d14-ec13c159dc2b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c86cb894-8d6f-4db3-9d14-ec13c159dc2b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c86cb894-8d6f-4db3-9d14-ec13c159dc2b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7308b0a-42d1-4082-aa17-d3c83864634d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7308b0a-42d1-4082-aa17-d3c83864634d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7308b0a-42d1-4082-aa17-d3c83864634d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to read text files\n",
        "def read_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Define paths\n",
        "articles_path = '/content/data/cnn_dailymail_dataset/Articles'\n",
        "summaries_path = '/content/data/cnn_dailymail_dataset/Summaries'\n",
        "\n",
        "# Get list of all files\n",
        "all_article_files = sorted(os.listdir(articles_path))\n",
        "all_summary_files = sorted(os.listdir(summaries_path))\n",
        "\n",
        "# Ensure both lists are of the same length\n",
        "assert len(all_article_files) == len(all_summary_files)"
      ],
      "metadata": {
        "id": "a5qFIKwhijVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set parameters for batch processing\n",
        "total_files = len(all_article_files)\n",
        "batch_size = int(0.1 * total_files)  # Process 10% of files at a time, adjust as needed\n",
        "article_min_words = 250\n",
        "summary_min_words = 10\n",
        "\n",
        "# Shuffle the files for randomness (optional)\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(total_files)\n",
        "\n",
        "# Initialize list to store batch-wise results\n",
        "batch_results = []\n",
        "\n",
        "# Batch-wise processing\n",
        "for i in range(0, total_files, batch_size):\n",
        "    print(f\"Processing batch {i // batch_size + 1}...\")\n",
        "\n",
        "    # Get the current batch of files\n",
        "    batch_indices = indices[i:i + batch_size]\n",
        "    batch_article_files = [all_article_files[idx] for idx in batch_indices]\n",
        "    batch_summary_files = [all_summary_files[idx] for idx in batch_indices]\n",
        "\n",
        "    # Initialize lists to store filtered articles and summaries\n",
        "    filtered_articles = []\n",
        "    filtered_summaries = []\n",
        "\n",
        "    # Process each article-summary pair in the current batch\n",
        "    for article_file, summary_file in zip(batch_article_files, batch_summary_files):\n",
        "        article_text = read_file(os.path.join(articles_path, article_file))\n",
        "        summary_text = read_file(os.path.join(summaries_path, summary_file))\n",
        "\n",
        "        # Check if both article and summary meet the minimum word count requirements\n",
        "        if len(article_text.split()) >= article_min_words and len(summary_text.split()) >= summary_min_words:\n",
        "            filtered_articles.append(article_text)\n",
        "            filtered_summaries.append(summary_text)\n",
        "\n",
        "    # Create a DataFrame for the current batch\n",
        "    batch_df = pd.DataFrame({'article': filtered_articles, 'summary': filtered_summaries})\n",
        "\n",
        "    # Drop duplicates within the current batch\n",
        "    batch_df.drop_duplicates(subset=['article', 'summary'], inplace=True)\n",
        "\n",
        "    # Store the filtered batch\n",
        "    batch_results.append(batch_df)\n",
        "\n",
        "    # Optionally, save the current batch to a CSV file\n",
        "    batch_df.to_csv(f'/content/drive/My Drive/CNN_Dataset_Preprocessing/batch_{i // batch_size + 1}.csv', index=False)\n",
        "\n",
        "# Combine all batches into one DataFrame\n",
        "combined_df = pd.concat(batch_results, ignore_index=True)\n",
        "\n",
        "# Further deduplicate after combining batches\n",
        "combined_df.drop_duplicates(subset=['article', 'summary'], inplace=True)\n",
        "print(f\"Total cleaned article-summary pairs: {len(combined_df)}\")\n",
        "\n",
        "# Save the final combined DataFrame to a CSV file\n",
        "combined_df.to_csv('/content/drive/My Drive/CNN_Dataset_Preprocessing/final_cleaned_dataset.csv', index=False)\n",
        "\n",
        "# Create directories to save the final cleaned text files\n",
        "os.makedirs('/content/final_cleaned_data/Articles', exist_ok=True)\n",
        "os.makedirs('/content/final_cleaned_data/Summaries', exist_ok=True)\n",
        "\n",
        "# Save the cleaned articles and summaries back to text files\n",
        "for index, row in combined_df.iterrows():\n",
        "    # Save article\n",
        "    article_filename = f\"/content/final_cleaned_data/Articles/article_{index}.txt\"\n",
        "    with open(article_filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(row['article'])\n",
        "\n",
        "    # Save summary\n",
        "    summary_filename = f\"/content/final_cleaned_data/Summaries/summary_{index}.txt\"\n",
        "    with open(summary_filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(row['summary'])\n",
        "\n",
        "print(\"Batch processing and filtering completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BadGxTXesMKW",
        "outputId": "b7d2247d-f157-439e-d1b4-0a88e5cab046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1...\n",
            "Processing batch 2...\n",
            "Processing batch 3...\n",
            "Processing batch 4...\n",
            "Processing batch 5...\n",
            "Processing batch 6...\n",
            "Processing batch 7...\n",
            "Processing batch 8...\n",
            "Processing batch 9...\n",
            "Processing batch 10...\n",
            "Processing batch 11...\n",
            "Total cleaned article-summary pairs: 269710\n",
            "Batch processing and filtering completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will do Quality-Based Filtering using:\n",
        "\n",
        "**Summarization Ratio**: To ensure that the summary is a meaningful representation of the article. For example, you can filter out pairs where the summary is too short relative to the article length.\n",
        "\n",
        "**Unique Words Count**: To ensure the content richness, we can filter out articles that have too few unique words, which might indicate redundancy or low-quality content.\n"
      ],
      "metadata": {
        "id": "EGj4Z8iKU_4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "6DbJV6yLVLQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the final cleaned dataset\n",
        "file_path = '/content/drive/My Drive/CNN_Dataset_Preprocessing/first_cleaned_dataset.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "RgFI-crpVShb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate summarization ratio\n",
        "def summarization_ratio(article, summary):\n",
        "    article_len = len(article.split())\n",
        "    summary_len = len(summary.split())\n",
        "    return summary_len / article_len if article_len > 0 else 0\n",
        "\n",
        "# Function to count unique words in a text\n",
        "def unique_word_count(text):\n",
        "    return len(set(text.split()))\n"
      ],
      "metadata": {
        "id": "NC-sRvcjVpTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define thresholds for filtering\n",
        "min_ratio = 0.05  # Summarization ratio threshold (summary should be at least 5% of the article length)\n",
        "min_unique_words = 50  # Minimum number of unique words in the article\n"
      ],
      "metadata": {
        "id": "FR8gyz_iVs7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Summarization Ratio Filter\n",
        "df['summarization_ratio'] = df.apply(lambda row: summarization_ratio(row['article'], row['summary']), axis=1)\n",
        "\n",
        "# Filter out pairs where the summarization ratio is below the threshold\n",
        "df = df[df['summarization_ratio'] >= min_ratio]\n"
      ],
      "metadata": {
        "id": "H8AkvbtoWOk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Apply Unique Words Count Filter\n",
        "df['unique_word_count'] = df['article'].apply(unique_word_count)\n",
        "\n",
        "# Filter out pairs where the article has fewer unique words than the threshold\n",
        "df = df[df['unique_word_count'] >= min_unique_words]\n"
      ],
      "metadata": {
        "id": "L4lwt7PMWTd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop temporary columns used for filtering\n",
        "df.drop(columns=['summarization_ratio', 'unique_word_count'], inplace=True)\n",
        "\n",
        "# Display the number of remaining pairs after filtering\n",
        "print(f\"Total pairs after Quality-Based Filtering: {len(df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9A9UsSMWedi",
        "outputId": "991e6ab7-384d-4d88-826a-dd419b02261b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs after Quality-Based Filtering: 213770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More quality filtering"
      ],
      "metadata": {
        "id": "RocrYKxeYIy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update thresholds for more aggressive filtering\n",
        "min_ratio = 0.10  # Increase summarization ratio to 10%\n",
        "min_unique_words = 100  # Increase unique words count to 100\n",
        "\n",
        "# Apply updated summarization ratio filter\n",
        "df['summarization_ratio'] = df.apply(lambda row: summarization_ratio(row['article'], row['summary']), axis=1)\n",
        "df_filtered = df[df['summarization_ratio'] >= min_ratio].copy()\n",
        "\n",
        "# Apply updated unique word count filter\n",
        "df_filtered['unique_word_count'] = df_filtered['article'].apply(unique_word_count)\n",
        "df_filtered = df_filtered[df_filtered['unique_word_count'] >= min_unique_words].copy()\n",
        "\n",
        "# Drop temporary columns\n",
        "df_filtered.drop(columns=['summarization_ratio', 'unique_word_count'], inplace=True)\n",
        "\n",
        "# Display updated counts\n",
        "print(f\"Total pairs after more aggressive Quality-Based Filtering: {len(df_filtered)}\")\n",
        "\n",
        "# Save the aggressively filtered dataset\n",
        "df_filtered.to_csv('/content/drive/My Drive/CNN_Dataset_Preprocessing/aggressively_filtered_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0w0oONGYIhx",
        "outputId": "1bf3b0a4-41da-4bd7-8d8a-bb3cf35111cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs after more aggressive Quality-Based Filtering: 78833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the aggressively filtered dataset\n",
        "df_filtered = pd.read_csv('/content/drive/My Drive/CNN_Dataset_Preprocessing/aggressively_filtered_dataset.csv')\n",
        "\n",
        "# Check the shape of the loaded dataset\n",
        "print(f\"Loaded dataset shape: {df_filtered.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmtTeqjiZ7r5",
        "outputId": "59dbe998-b220-40e6-c3b8-9425112951c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset shape: (78833, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create directories for articles and summaries\n",
        "os.makedirs('/content/drive/My Drive/CNN_Dataset_Preprocessing/Filtered_Dataset/Articles', exist_ok=True)\n",
        "os.makedirs('/content/drive/My Drive/CNN_Dataset_Preprocessing/Filtered_Dataset/Summaries', exist_ok=True)\n",
        "\n",
        "# Save each article and summary as separate text files\n",
        "for idx, row in df_filtered.iterrows():\n",
        "    article_filename = f\"/content/drive/My Drive/CNN_Dataset_Preprocessing/Filtered_Dataset/Articles/article_{idx}.txt\"\n",
        "    summary_filename = f\"/content/drive/My Drive/CNN_Dataset_Preprocessing/Filtered_Dataset/Summaries/summary_{idx}.txt\"\n",
        "\n",
        "    with open(article_filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(row['article'])\n",
        "\n",
        "    with open(summary_filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(row['summary'])\n",
        "\n",
        "print(\"Articles and Summaries saved successfully in separate folders!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKjFr4gqagwC",
        "outputId": "10ea4d93-a3fa-4fd2-e587-6e0c29f96cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles and Summaries saved successfully in separate folders!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "tutbaw9RZ3WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMptrzboicZP",
        "outputId": "da46bdfd-dd0b-4768-ffeb-b441d185e282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "4XA9DAjpglgX",
        "outputId": "6aea93d1-2ec9-49a9-f36e-97d12e451bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.1.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-75.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-75.4.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-24.3.1 setuptools-75.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "setuptools"
                ]
              },
              "id": "b4a6d80016e142749c2e3e6e58390d25"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk scikit-learn spacy gensim pyspellchecker langdetect\n",
        "\n",
        "# Download additional NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')  # Make sure this is correctly downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Download Spacy model for NER\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g151a7c1gVxZ",
        "outputId": "f23c5797-0ffd-4a8d-82af-09f5d09d7dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries after downloading\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from spellchecker import SpellChecker\n",
        "from langdetect import detect, LangDetectException\n",
        "from collections import Counter\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "# Load Spacy's English NER model\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize spell checker, lemmatizer, stemmer, and stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "spell = SpellChecker()\n"
      ],
      "metadata": {
        "id": "uiFqp17zhEue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Cleaning"
      ],
      "metadata": {
        "id": "hseTatKuivi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercase Transformation: Convert text to lowercase to ensure uniformity."
      ],
      "metadata": {
        "id": "-_cK2QuUi9Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_lowercase(text):\n",
        "    \"\"\"Convert text to lowercase.\"\"\"\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "fH3SoFTQiPTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove URLs and Emails: Eliminate any web addresses or email addresses if present."
      ],
      "metadata": {
        "id": "P7jJZBtgjG4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs from the text.\"\"\"\n",
        "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    \"\"\"Remove email addresses from the text.\"\"\"\n",
        "    return re.sub(r'\\S+@\\S+', '', text)\n"
      ],
      "metadata": {
        "id": "VphZ5I0sjH8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expand Contractions: Convert contractions to their full forms (e.g., \"can't\" to \"cannot\") to\n",
        "ensure consistency."
      ],
      "metadata": {
        "id": "rrbnELg7jVXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    \"\"\"Expand common contractions.\"\"\"\n",
        "    contractions = {\n",
        "        \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\",\n",
        "        \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
        "    }\n",
        "    for contraction, full_form in contractions.items():\n",
        "        text = re.sub(contraction, full_form, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "DlPRS6-EjMLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Special Characters and Punctuation: Remove non-alphanumeric characters (like @, #,\n",
        "&, etc.), punctuation, and any irrelevant symbols."
      ],
      "metadata": {
        "id": "ucQgkcwSjg5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    \"\"\"Remove special characters and punctuation.\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
      ],
      "metadata": {
        "id": "OCTu0aqBjfs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Normalization: Standardize text by removing extra spaces, tabs, and newline characters."
      ],
      "metadata": {
        "id": "gYW8t1SXjw9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_spaces(text):\n",
        "    \"\"\"Remove extra spaces, tabs, and newlines.\"\"\"\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "kLZ-gX6AjxX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct Misspellings"
      ],
      "metadata": {
        "id": "xhR4ZX2DkCdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spellings(text):\n",
        "    \"\"\"Correct misspelled words.\"\"\"\n",
        "    corrected_words = []\n",
        "    for word in text.split():\n",
        "        correction = spell.correction(word)\n",
        "        # If correction is None, use the original word\n",
        "        corrected_words.append(correction if correction else word)\n",
        "    return ' '.join(corrected_words)\n"
      ],
      "metadata": {
        "id": "GRN-rB-4j64g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stop Words: remove common but less\n",
        "informative words (e.g., \"is,\" \"and,\" \"the\")."
      ],
      "metadata": {
        "id": "pycyFC_NkK3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords from the text.\"\"\"\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])"
      ],
      "metadata": {
        "id": "-c7pufdnkFaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Apply all cleaning functions in sequence.\"\"\"\n",
        "    text = convert_to_lowercase(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emails(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    text = correct_spellings(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "EdYiQwVpkTsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/My Drive/CNN_Dataset_Preprocessing/aggressively_filtered_dataset.csv')\n",
        "\n",
        "# Clean articles and summaries\n",
        "df['article_cleaned'] = df['article'].apply(clean_text)\n",
        "df['summary_cleaned'] = df['summary'].apply(clean_text)\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv('/content/drive/My Drive/CNN_Dataset_Preprocessing/cleaned_dataset.csv', index=False)\n",
        "\n",
        "print(f\"Total pairs after comprehensive cleaning: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "DUUBS4gMlQPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/CNN_Dataset_Preprocessing/cleaned_sampled_dataset.csv')\n"
      ],
      "metadata": {
        "id": "9VDxaIjz-0yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "def sentence_tokenize(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def spacy_sentence_tokenize(text):\n",
        "    doc = spacy_nlp(text)\n",
        "    return [sent.text for sent in doc.sents]\n",
        "\n",
        "# Word Tokenization\n",
        "def word_tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Define a Spacy-based word tokenizer\n",
        "def spacy_word_tokenize(text):\n",
        "    doc = spacy_nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Lemmatization\n",
        "def lemmatize_words(words):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Stemming\n",
        "def stem_words(words):\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "# POS Tagging using Spacy\n",
        "def spacy_pos_tagging(words_list):\n",
        "    return [(token.text, token.pos_) for token in spacy_nlp(\" \".join(words_list))]\n",
        "\n",
        "# Named Entity Recognition using Spacy\n",
        "def named_entity_recognition(text):\n",
        "    doc = spacy_nlp(text)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "def tfidf_vectorize(text_series):\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    return vectorizer.fit_transform(text_series).toarray()\n",
        "\n",
        "# Word2Vec Embeddings function using Spacy\n",
        "def generate_word2vec_embeddings(text_series):\n",
        "    tokenized_sentences = []\n",
        "    for text in text_series:\n",
        "        # Use Spacy for sentence tokenization\n",
        "        doc = spacy_nlp(text)\n",
        "        # Tokenize each sentence\n",
        "        tokenized_sentences.append([token.text for token in doc if not token.is_punct])\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
        "    return word2vec_model\n"
      ],
      "metadata": {
        "id": "_KcfQ_cxBwOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "df['article_sentences'] = df['article_cleaned'].apply(spacy_sentence_tokenize)\n",
        "df['summary_sentences'] = df['summary_cleaned'].apply(spacy_sentence_tokenize)\n",
        "\n",
        "# Apply Spacy-based word tokenizer\n",
        "df['article_words'] = df['article_sentences'].apply(lambda sentences: [spacy_word_tokenize(sent) for sent in sentences])\n",
        "df['summary_words'] = df['summary_sentences'].apply(lambda sentences: [spacy_word_tokenize(sent) for sent in sentences])\n",
        "\n",
        "# Lemmatization\n",
        "df['article_words_lemmatized'] = df['article_words'].apply(lambda words_list: [lemmatize_words(words) for words in words_list])\n",
        "df['summary_words_lemmatized'] = df['summary_words'].apply(lambda words_list: [lemmatize_words(words) for words in words_list])\n",
        "\n",
        "# Stemming (optional)\n",
        "df['article_words_stemmed'] = df['article_words'].apply(lambda words_list: [stem_words(words) for words in words_list])\n",
        "\n",
        "# Named Entity Recognition\n",
        "df['article_entities'] = df['article_cleaned'].apply(named_entity_recognition)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_matrix = tfidf_vectorize(df['article_cleaned'])\n",
        "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
        "\n",
        "# Apply the updated function\n",
        "word2vec_model = generate_word2vec_embeddings(df['article_cleaned'])\n",
        "\n",
        "# Apply the updated Spacy-based POS tagging\n",
        "df['article_pos'] = df['article_words'].apply(lambda words_list: [spacy_pos_tagging(words) for words in words_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rfs-q9TB2-N",
        "outputId": "38ef3acd-17e5-47ca-95e2-e3f08676872c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix Shape: (1000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path where you want to save the cleaned dataset\n",
        "final_save_path = '/content/drive/My Drive/CNN_Dataset_Preprocessing/final_cleaned_dataset.csv'\n",
        "\n",
        "# Save the fully processed DataFrame to CSV\n",
        "df.to_csv(final_save_path, index=False)\n",
        "\n",
        "print(f\"Final cleaned dataset saved successfully to: {final_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbzwMfDUJp_5",
        "outputId": "b2f7ac1f-3523-4d5c-8df4-b5454acaa72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final cleaned dataset saved successfully to: /content/drive/My Drive/CNN_Dataset_Preprocessing/final_cleaned_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Specify the path\n",
        "tfidf_save_path = '/content/drive/My Drive/CNN_Dataset_Preprocessing/tfidf_matrix.npy'\n",
        "\n",
        "# Save as a NumPy array\n",
        "np.save(tfidf_save_path, tfidf_matrix)\n",
        "\n",
        "print(f\"TF-IDF matrix saved successfully to: {tfidf_save_path}\")\n"
      ],
      "metadata": {
        "id": "mDnnOzyMNEm0",
        "outputId": "6340144c-5c5a-43a5-b4db-e19cca961bfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix saved successfully to: /content/drive/My Drive/CNN_Dataset_Preprocessing/tfidf_matrix.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path\n",
        "word2vec_save_path = '/content/drive/My Drive/CNN_Dataset_Preprocessing/word2vec_model.model'\n",
        "\n",
        "# Save the Word2Vec model\n",
        "word2vec_model.save(word2vec_save_path)\n",
        "\n",
        "print(f\"Word2Vec model saved successfully to: {word2vec_save_path}\")\n"
      ],
      "metadata": {
        "id": "iifkUG0pNGjb",
        "outputId": "461c58eb-ac7e-45c2-8e3c-fbeeef462c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model saved successfully to: /content/drive/My Drive/CNN_Dataset_Preprocessing/word2vec_model.model\n"
          ]
        }
      ]
    }
  ]
}