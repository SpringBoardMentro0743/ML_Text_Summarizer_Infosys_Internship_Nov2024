# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15FhZw6FvH6wYjy9-sE0XKt-FZor-oirW
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
!pip install langdetect
!pip install python-docx
# Required Libraries
import os
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from langdetect import detect
import docx  # Importing python-docx for saving to Word

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Synonym Replacement Dictionary
synonym_dict = {
    "NLP": "Natural Language Processing",
    "AI": "Artificial Intelligence",
    "ML": "Machine Learning",
    "stats": "statistics",
    # Add more terms as needed
}

# 1. Clean Text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove special characters
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text)  # Remove single characters
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text

# 2. Remove Stop Words
stop_words = set(stopwords.words('english'))
def remove_stop_words(text):
    tokens = word_tokenize(text)
    filtered_words = [word for word in tokens if word not in stop_words]
    return ' '.join(filtered_words)

# 3. Lemmatize Text
lemmatizer = WordNetLemmatizer()
def lemmatize_text(text):
    tokens = word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(lemmatized_words)

# 4. Detect Non-English Text
def is_english(text):
    try:
        return detect(text) == 'en'
    except:
        return False

# 5. Tokenization: Sentence and Word Tokenization
def tokenize_text(text):
    sentences = nltk.sent_tokenize(text)
    words = word_tokenize(text)
    return sentences, words

# 6. POS Tagging and Named Entity Recognition (NER)
def pos_tagging(text):
    tokens = word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    return pos_tags

def named_entity_recognition(text):
    tokens = word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    ner_tree = nltk.ne_chunk(pos_tags, binary=True)  # Binary=True identifies only named entities
    return ner_tree

# 7. Synonym Replacement
def replace_synonyms(text):
    words = text.split()
    replaced_text = ' '.join([synonym_dict.get(word, word) for word in words])
    return replaced_text

# 8. Vectorization using TF-IDF
def vectorize_tfidf(documents):
    tfidf = TfidfVectorizer()
    tfidf_matrix = tfidf.fit_transform(documents)
    return tfidf_matrix, tfidf.get_feature_names_out()

# 9. Padding and Truncation
def pad_sequences_data(documents, max_length=50):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(documents)
    sequences = tokenizer.texts_to_sequences(documents)
    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded_sequences

# 10. Combined Preprocessing Function
def preprocess_text(text):
    text = clean_text(text)
    text = remove_stop_words(text)
    text = lemmatize_text(text)
    text = replace_synonyms(text)  # Apply synonym replacement
    return text

# 11. Load Documents from Google Drive Folder
folder_path = '/content/drive/My Drive/part3'  # Update this path if needed
file_names = os.listdir(folder_path)

# Create a new Word document to store results
doc = docx.Document()
doc.add_heading('Preprocessed Data from part3', 0)
doc.add_paragraph('This document contains preprocessed text data.')

# Read and process files
documents = []
valid_file_names = []

for file_name in file_names:
    file_path = os.path.join(folder_path, file_name)
    if file_name.endswith('.txt') and os.path.isfile(file_path):  # Only process .txt files
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                valid_file_names.append(file_name)

                # Preprocess each document
                cleaned_text = preprocess_text(text)

                # Check if text is in English
                if is_english(cleaned_text):
                    documents.append(cleaned_text)
        except UnicodeDecodeError:
            # Handle potential encoding issues
            with open(file_path, 'r', encoding='ISO-8859-1') as file:
                text = file.read()
                valid_file_names.append(file_name)

                # Preprocess each document
                cleaned_text = preprocess_text(text)

                # Check if text is in English
                if is_english(cleaned_text):
                    documents.append(cleaned_text)

# Debugging: Check if documents were successfully processed
doc.add_paragraph(f"Total documents processed: {len(documents)}")
if len(documents) > 0:
    # Display preprocessed text samples
    for i in range(min(3, len(documents))):  # To avoid index error
        doc.add_paragraph(f"File: {valid_file_names[i]}")
        doc.add_paragraph(f"Preprocessed Text: {documents[i]}")

# 12. Save Preprocessed Documents to Google Drive
output_folder = '/content/drive/My Drive/part3_preprocessed_docs'
os.makedirs(output_folder, exist_ok=True)

for i, doc_text in enumerate(documents):
    doc = docx.Document()
    doc.add_heading(f"Document {i+1}: {valid_file_names[i]}", 0)
    doc.add_paragraph(doc_text)
    doc_file_path = os.path.join(output_folder, f"preprocessed_{valid_file_names[i]}.docx")
    doc.save(doc_file_path)
    print(f"Saved preprocessed document {i+1} as {doc_file_path}")

# Save summary document
doc.save('/content/drive/My Drive/preprocessed_data_summary.docx')

print("All preprocessed documents saved successfully!")