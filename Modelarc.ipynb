{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24578,
     "status": "ok",
     "timestamp": 1734678721349,
     "user": {
      "displayName": "Kurri Sravani",
      "userId": "17580324738064493146"
     },
     "user_tz": -330
    },
    "id": "xXXG_8mb5DVO",
    "outputId": "38345936-7bfb-4b30-e01e-a13f3b8ebb7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text to summarize: Artificial Intelligence (AI) is one of the most transformative technologies of the 21st century, reshaping industries, societies, and daily life. At its core, AI refers to the simulation of human intelligence by machines that are programmed to think, learn, and make decisions. From self-driving cars to virtual assistants, AI has permeated various aspects of our existence, promising both immense benefits and significant challenges. This essay explores the evolution, applications, benefits, ethical concerns, and future potential of AI. The concept of artificial intelligence dates back to ancient times, with myths and stories about mechanical beings endowed with human-like intelligence. However, the formal study of AI began in the mid-20th century. In 1956, the term \"artificial intelligence\" was coined at the Dartmouth Conference, marking the birth of AI as a field of study. Early AI research focused on symbolic reasoning and problem-solving. Pioneering programs, such as the Logic Theorist and ELIZA, demonstrated the potential of machines to mimic human thought processes. However, limited computational power and data availability hindered progress. The advent of powerful computers, massive datasets, and advanced algorithms in the late 20th century reignited interest in AI, leading to the development of machine learning and deep learning.\n",
      "Enter the maximum number of sentences to summarize to (default is 3): 5\n",
      "Summary:\n",
      "The advent of powerful computers, massive datasets, and advanced algorithms in the late 20th century reignited interest in AI, leading to the development of machine learning and deep learning. In 1956, the term \"artificial intelligence\" was coined at the Dartmouth Conference, marking the birth of AI as a field of study. Artificial Intelligence (AI) is one of the most transformative technologies of the 21st century, reshaping industries, societies, and daily life.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from heapq import nlargest\n",
    "\n",
    "# Download the NLTK punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def nltk_summarize(text, num_sentences=3):\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Create a frequency table of words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    word_frequencies = {}\n",
    "    for word in words:\n",
    "        if word.isalnum():\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "\n",
    "    # Normalize frequencies\n",
    "    max_freq = max(word_frequencies.values())  # Changed variable name from 'max' to 'max_freq'\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] /= max_freq\n",
    "        # Score sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in nltk.word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies:\n",
    "                sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_frequencies[word]\n",
    "\n",
    "    # Select top N sentences\n",
    "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    return ' '.join(summary_sentences)\n",
    "    # Get user input\n",
    "text_input = input(\"Enter text to summarize: \")\n",
    "max_sentences_input = input(\"Enter the maximum number of sentences to summarize to (default is 3): \")\n",
    "\n",
    "try:\n",
    "    # Convert max_sentences_input to an integer\n",
    "    max_sentences = int(max_sentences_input)\n",
    "except ValueError:\n",
    "    max_sentences = 3  # Default value if input is invalid\n",
    "    print(\"Invalid input for number of sentences. Using default value of 3.\")\n",
    "\n",
    "num_sentences = min(3, max_sentences)  # Set the number of sentences to summarize\n",
    "\n",
    "# Generate summary\n",
    "if text_input.strip():\n",
    "    summary = nltk_summarize(text_input, num_sentences)\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"Please enter some text to summarize.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN3tGYBREc2E5ZkLzMJHteV",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1nV0BpwTBoAriEjs14W6unFMY_fN7grue",
     "timestamp": 1734678764347
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
